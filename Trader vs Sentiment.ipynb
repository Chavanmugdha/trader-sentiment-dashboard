{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMl2WLao9L8XZOEnAFtV4A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dnf6sI4hx5gE","executionInfo":{"status":"ok","timestamp":1757428942189,"user_tz":-330,"elapsed":18277,"user":{"displayName":"Mugdha ChavanB72","userId":"11346652250748895350"}},"outputId":"27bb255e-c8f4-4b5a-920b-b95e0dbbdad7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing required packages (this may take a minute)...\n","Loading /content/historical_data.csv and /content/fear_greed_index.csv\n","Trades shape: (211224, 16)\n","Sentiment shape: (2644, 4)\n","Parsed trades time from column: time\n","Warning: 211224 trade rows have missing/invalid timestamps. They will be dropped.\n","Merged shape: (0, 20)\n","Sentiment distribution in merged:\n","Series([], Name: count, dtype: int64)\n"]},{"output_type":"stream","name":"stderr","text":["DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to load 'maxp'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n","INFO:fontTools.subset:maxp pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n","DEBUG:fontTools.subset.timer:Took 0.005s to load 'cmap'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n","INFO:fontTools.subset:cmap pruned\n","INFO:fontTools.subset:fpgm dropped\n","INFO:fontTools.subset:prep dropped\n","INFO:fontTools.subset:cvt  dropped\n","INFO:fontTools.subset:kern dropped\n","DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n","INFO:fontTools.subset:post pruned\n","INFO:fontTools.subset:GPOS dropped\n","INFO:fontTools.subset:GSUB dropped\n","DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n","DEBUG:fontTools.subset.timer:Took 0.006s to load 'glyf'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n","INFO:fontTools.subset:Added gid0 to subset\n","INFO:fontTools.subset:Closing glyph list over 'glyf': 38 glyphs before\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'B', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'ampersand', 'b', 'c', 'd', 'e', 'f', 'g', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 'seven', 't', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'y', 'zero']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 9, 16, 19, 26, 36, 37, 47, 48, 50, 51, 53, 54, 55, 58, 66, 68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92]\n","INFO:fontTools.subset:Closed glyph list over 'glyf': 38 glyphs after\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'B', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'ampersand', 'b', 'c', 'd', 'e', 'f', 'g', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 'seven', 't', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'y', 'zero']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 9, 16, 19, 26, 36, 37, 47, 48, 50, 51, 53, 54, 55, 58, 66, 68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92]\n","DEBUG:fontTools.subset.timer:Took 0.005s to close glyph list over 'glyf'\n","INFO:fontTools.subset:Retaining 38 glyphs\n","INFO:fontTools.subset:head subsetting not needed\n","INFO:fontTools.subset:hhea subsetting not needed\n","INFO:fontTools.subset:maxp subsetting not needed\n","INFO:fontTools.subset:OS/2 subsetting not needed\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n","DEBUG:fontTools.subset.timer:Took 0.003s to subset 'hmtx'\n","INFO:fontTools.subset:hmtx subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n","INFO:fontTools.subset:cmap subsetted\n","INFO:fontTools.subset:loca subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n","INFO:fontTools.subset:post subsetted\n","INFO:fontTools.subset:gasp subsetting not needed\n","INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n","DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to subset 'GDEF'\n","INFO:fontTools.subset:GDEF subsetted\n","INFO:fontTools.subset:name subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n","INFO:fontTools.subset:glyf subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n","INFO:fontTools.subset:head pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n","INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n","INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n","DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n","INFO:fontTools.subset:GDEF pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n","DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n","INFO:fontTools.subset:name pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to load 'maxp'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n","INFO:fontTools.subset:maxp pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n","DEBUG:fontTools.subset.timer:Took 0.004s to load 'cmap'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n","INFO:fontTools.subset:cmap pruned\n","INFO:fontTools.subset:fpgm dropped\n"]},{"output_type":"stream","name":"stdout","text":["Not enough data for predictive model (need >=50 rows with Fear/Greed).\n","Saved HTML report to: /content/trader_sentiment_results/analysis_report.html\n"]},{"output_type":"stream","name":"stderr","text":["INFO:fontTools.subset:prep dropped\n","INFO:fontTools.subset:cvt  dropped\n","INFO:fontTools.subset:kern dropped\n","DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n","INFO:fontTools.subset:post pruned\n","INFO:fontTools.subset:GPOS dropped\n","INFO:fontTools.subset:GSUB dropped\n","DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n","DEBUG:fontTools.subset.timer:Took 0.005s to load 'glyf'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n","INFO:fontTools.subset:Added gid0 to subset\n","INFO:fontTools.subset:Closing glyph list over 'glyf': 55 glyphs before\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'C', 'F', 'G', 'I', 'L', 'N', 'P', 'S', 'T', 'U', 'V', 'a', 'b', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'eight', 'equal', 'f', 'five', 'four', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'period', 'plus', 'r', 's', 'seven', 'six', 't', 'two', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'y', 'z', 'zero']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 11, 12, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 38, 41, 42, 44, 47, 49, 51, 54, 55, 56, 57, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 93, 535]\n","INFO:fontTools.subset:Closed glyph list over 'glyf': 55 glyphs after\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'C', 'F', 'G', 'I', 'L', 'N', 'P', 'S', 'T', 'U', 'V', 'a', 'b', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'eight', 'equal', 'f', 'five', 'four', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'period', 'plus', 'r', 's', 'seven', 'six', 't', 'two', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'y', 'z', 'zero']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 11, 12, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 38, 41, 42, 44, 47, 49, 51, 54, 55, 56, 57, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 93, 535]\n","DEBUG:fontTools.subset.timer:Took 0.004s to close glyph list over 'glyf'\n","INFO:fontTools.subset:Retaining 55 glyphs\n","INFO:fontTools.subset:head subsetting not needed\n","INFO:fontTools.subset:hhea subsetting not needed\n","INFO:fontTools.subset:maxp subsetting not needed\n","INFO:fontTools.subset:OS/2 subsetting not needed\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n","DEBUG:fontTools.subset.timer:Took 0.003s to subset 'hmtx'\n","INFO:fontTools.subset:hmtx subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n","INFO:fontTools.subset:cmap subsetted\n","INFO:fontTools.subset:loca subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n","INFO:fontTools.subset:post subsetted\n","INFO:fontTools.subset:gasp subsetting not needed\n","INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n","DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to subset 'GDEF'\n","INFO:fontTools.subset:GDEF subsetted\n","INFO:fontTools.subset:name subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.001s to subset 'glyf'\n","INFO:fontTools.subset:glyf subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n","INFO:fontTools.subset:head pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n","INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1, 31]\n","INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n","DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n","INFO:fontTools.subset:GDEF pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n","DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n","INFO:fontTools.subset:name pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to load 'maxp'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n","INFO:fontTools.subset:maxp pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n","DEBUG:fontTools.subset.timer:Took 0.004s to load 'cmap'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n","INFO:fontTools.subset:cmap pruned\n","INFO:fontTools.subset:fpgm dropped\n","INFO:fontTools.subset:prep dropped\n","INFO:fontTools.subset:cvt  dropped\n","DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n","INFO:fontTools.subset:post pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n","DEBUG:fontTools.subset.timer:Took 0.005s to load 'glyf'\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n","INFO:fontTools.subset:Added gid0 to subset\n","INFO:fontTools.subset:Closing glyph list over 'glyf': 12 glyphs before\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'S', 'bracketleft', 'bracketright', 'comma', 'e', 'i', 'parenleft', 'parenright', 'r', 's', 'uni00A0']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 11, 12, 15, 54, 62, 64, 72, 76, 85, 86]\n","INFO:fontTools.subset:Closed glyph list over 'glyf': 12 glyphs after\n","INFO:fontTools.subset:Glyph names: ['.notdef', 'S', 'bracketleft', 'bracketright', 'comma', 'e', 'i', 'parenleft', 'parenright', 'r', 's', 'uni00A0']\n","INFO:fontTools.subset:Glyph IDs:   [0, 3, 11, 12, 15, 54, 62, 64, 72, 76, 85, 86]\n","DEBUG:fontTools.subset.timer:Took 0.004s to close glyph list over 'glyf'\n","INFO:fontTools.subset:Retaining 12 glyphs\n","INFO:fontTools.subset:head subsetting not needed\n","INFO:fontTools.subset:hhea subsetting not needed\n","INFO:fontTools.subset:maxp subsetting not needed\n","INFO:fontTools.subset:OS/2 subsetting not needed\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n","DEBUG:fontTools.subset.timer:Took 0.003s to subset 'hmtx'\n","INFO:fontTools.subset:hmtx subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n","INFO:fontTools.subset:cmap subsetted\n","INFO:fontTools.subset:loca subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n","INFO:fontTools.subset:post subsetted\n","INFO:fontTools.subset:gasp subsetting not needed\n","INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n","DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n","DEBUG:fontTools.subset.timer:Took 0.002s to subset 'GDEF'\n","INFO:fontTools.subset:GDEF subsetted\n","INFO:fontTools.subset:name subsetting not needed\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n","INFO:fontTools.subset:glyf subsetted\n","DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n","INFO:fontTools.subset:head pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n","INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n","INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n","INFO:fontTools.subset:glyf pruned\n","DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n","INFO:fontTools.subset:GDEF pruned\n","DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n","DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n","DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n","DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n","INFO:fontTools.subset:name pruned\n"]},{"output_type":"stream","name":"stdout","text":["Saved PDF report via WeasyPrint to: /content/trader_sentiment_results/analysis_report.pdf\n","Saved merged CSV to: /content/trader_sentiment_results/merged_trades_sentiment.csv\n","Saved Streamlit app script to: /content/trader_sentiment_results/dashboard_app.py\n","To run the dashboard locally or in Colab (if supported), run:\n","  streamlit run /content/trader_sentiment_results/dashboard_app.py\n","\n","=== Finished ===\n","Outputs saved to directory: /content/trader_sentiment_results\n","Key files:\n"," - HTML report: /content/trader_sentiment_results/analysis_report.html\n"," - PDF report: /content/trader_sentiment_results/analysis_report.pdf\n"," - Merged CSV: /content/trader_sentiment_results/merged_trades_sentiment.csv\n"," - Sentiment aggregates CSV: /content/trader_sentiment_results/sentiment_aggregates.csv\n"," - Streamlit app: /content/trader_sentiment_results/dashboard_app.py\n"," - Interactive Plotly HTML: /content/trader_sentiment_results/plotly_pnl_by_sentiment.html\n","\n","Open the HTML report to inspect visuals immediately. If PDF conversion fails, you still have the HTML and PNG images.\n"]}],"source":["# -----------------------------\n","# Trader vs Sentiment\n","# -----------------------------\n","#\n","# It will:\n","#  - install required packages,\n","#  - load /mnt/data/historical_data.csv and /mnt/data/fear_greed_index.csv (fallback to cwd),\n","#  - analyze, plot, create HTML report with embedded images,\n","#  - convert HTML->PDF (WeasyPrint preferred, ReportLab fallback),\n","#  - save outputs and a Streamlit dashboard script.\n","# -----------------------------\n","\n","# -----------------------------\n","# 0) Install system deps & pip packages (Colab friendly)\n","# -----------------------------\n","import sys, os\n","print(\"Installing required packages (this may take a minute)...\", flush=True)\n","# apt-get deps needed by weasyprint (Colab)\n","os.system(\"apt-get update -qq > /dev/null\")\n","os.system(\"apt-get install -y -qq libpangocairo-1.0-0 libpango1.0-0 libcairo2 libgdk-pixbuf2.0-0 libffi-dev shared-mime-info > /dev/null || true\")\n","# pip installs\n","os.system(\"pip install --quiet pandas numpy matplotlib seaborn plotly weasyprint pillow reportlab streamlit nbconvert jinja2 scikit-learn\")\n","\n","# -----------------------------\n","# 1) Imports\n","# -----------------------------\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","from scipy import stats\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report, roc_auc_score\n","from io import BytesIO\n","import base64\n","from datetime import datetime\n","from reportlab.lib.pagesizes import letter\n","from reportlab.pdfgen import canvas\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","sns.set(style='whitegrid')\n","\n","# Try importing weasyprint (used for HTML -> PDF)\n","use_weasy = True\n","try:\n","    from weasyprint import HTML\n","except Exception as e:\n","    print(\"WeasyPrint import failed (will attempt fallback). Error:\", e)\n","    use_weasy = False\n","\n","# -----------------------------\n","# 2) Paths and load CSVs\n","# -----------------------------\n","out_dir = '/content/trader_sentiment_results'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# Candidate paths (the environment indicated the files at /mnt/data)\n","path1 = '/content/historical_data.csv'\n","path2 = '/content/fear_greed_index.csv'\n","fallback1 = 'historical_data.csv'\n","fallback2 = 'fear_greed_index.csv'\n","\n","def load_csv_try(path_a, path_b):\n","    if os.path.exists(path_a) and os.path.exists(path_b):\n","        print(f\"Loading {path_a} and {path_b}\")\n","        return pd.read_csv(path_a), pd.read_csv(path_b)\n","    elif os.path.exists(fallback1) and os.path.exists(fallback2):\n","        print(f\"Loading {fallback1} and {fallback2} from working dir\")\n","        return pd.read_csv(fallback1), pd.read_csv(fallback2)\n","    else:\n","        raise FileNotFoundError(f\"Could not find datasets at {path_a}/{path_b} or {fallback1}/{fallback2}. Please upload them to Colab.\")\n","\n","trades, sentiment = load_csv_try(path1, path2)\n","print(\"Trades shape:\", trades.shape)\n","print(\"Sentiment shape:\", sentiment.shape)\n","\n","# -----------------------------\n","# 3) Basic cleaning & datetime parsing\n","# -----------------------------\n","# Normalize column names\n","trades.columns = trades.columns.str.strip().str.lower().str.replace(' ', '_')\n","sentiment.columns = sentiment.columns.str.strip().str.lower().str.replace(' ', '_')\n","\n","# Identify time column in trades\n","time_col_candidates = ['time', 'timestamp', 'datetime', 'date']\n","trades['time'] = None\n","for c in time_col_candidates:\n","    if c in trades.columns:\n","        try:\n","            trades['time'] = pd.to_datetime(trades[c], errors='coerce')\n","            print(f\"Parsed trades time from column: {c}\")\n","            break\n","        except Exception:\n","            pass\n","# If still none, try numeric epoch -> datetime\n","if trades['time'].isna().all() and 'time' in trades.columns:\n","    trades['time'] = pd.to_datetime(trades['time'], unit='s', errors='coerce') # Added unit='s' for potential epoch conversion\n","\n","# Sentiment: identify date and classification\n","date_col = None\n","for c in ['date', 'time', 'timestamp', 'datetime']:\n","    if c in sentiment.columns:\n","        date_col = c\n","        break\n","if date_col is None:\n","    raise ValueError(\"No date column found in sentiment CSV (expected 'date'/'time'/'timestamp').\")\n","\n","sentiment['date_parsed'] = pd.to_datetime(sentiment[date_col], errors='coerce').dt.date\n","\n","# Identify classification column\n","possible_class_cols = [c for c in sentiment.columns if 'class' in c or 'fear' in c or 'greed' in c or 'index' in c]\n","if 'classification' in sentiment.columns:\n","    sentiment['classification'] = sentiment['classification']\n","elif len(possible_class_cols) > 0:\n","    # choose first plausible (prefer exact names)\n","    chosen = possible_class_cols[0]\n","    sentiment['classification'] = sentiment[chosen]\n","else:\n","    # fallback: if second column looks like labels\n","    cols = list(sentiment.columns)\n","    if len(cols) >= 2:\n","        sentiment['classification'] = sentiment[cols[1]]\n","    else:\n","        raise ValueError(\"Could not find a classification column in sentiment CSV.\")\n","\n","# Normalize classification strings -> Fear/Greed/Unknown\n","sentiment['classification'] = sentiment['classification'].astype(str).str.strip().str.title()\n","sentiment = sentiment[['date_parsed','classification']].rename(columns={'date_parsed':'date'})\n","\n","# Create trade_date column in trades (date only)\n","trades['trade_date'] = trades['time'].dt.date\n","\n","# Drop trades with missing time (or keep with Unknown later)\n","missing_times = trades['time'].isna().sum()\n","if missing_times > 0:\n","    print(f\"Warning: {missing_times} trade rows have missing/invalid timestamps. They will be dropped.\")\n","    trades = trades.dropna(subset=['time']).copy()\n","\n","# -----------------------------\n","# 4) Merge datasets by date\n","# -----------------------------\n","merged = trades.merge(sentiment, left_on='trade_date', right_on='date', how='left')\n","merged['classification'] = merged['classification'].fillna('Unknown')\n","\n","print(\"Merged shape:\", merged.shape)\n","print(\"Sentiment distribution in merged:\")\n","print(merged['classification'].value_counts())\n","\n","# -----------------------------\n","# 5) Feature engineering\n","# -----------------------------\n","# Closed PnL column: try several variants\n","pnl_col = None\n","for c in ['closedpnl', 'closed_pnl', 'closedPnL', 'closedPnL']:\n","    if c in merged.columns:\n","        pnl_col = c\n","        break\n","# If no closed PnL, try 'pnl' or 'profit'\n","if pnl_col is None:\n","    for c in ['pnl','profit','realized_pnl']:\n","        if c in merged.columns:\n","            pnl_col = c\n","            break\n","if pnl_col is None:\n","    print(\"No obvious closed PnL column found; creating 'pnl' = 0 for all (please update mapping).\")\n","    merged['pnl'] = 0.0\n","else:\n","    merged['pnl'] = pd.to_numeric(merged[pnl_col], errors='coerce').fillna(0.0)\n","\n","# size & leverage\n","# Check if 'size' column exists before processing\n","if 'size' in merged.columns:\n","    merged['size'] = pd.to_numeric(merged['size'], errors='coerce').fillna(0.0)\n","else:\n","    merged['size'] = 0.0 # Create column with default 0 if not found\n","\n","# Check if 'leverage' column exists before processing\n","if 'leverage' in merged.columns:\n","    merged['leverage'] = pd.to_numeric(merged['leverage'], errors='coerce').fillna(0.0)\n","else:\n","    merged['leverage'] = 0.0 # Create column with default 0 if not found\n","\n","\n","# side normalization\n","if 'side' in merged.columns:\n","    merged['side'] = merged['side'].astype(str).str.lower()\n","else:\n","    merged['side'] = ''\n","\n","# win indicator\n","merged['win'] = (merged['pnl'] > 0).astype(int)\n","\n","# leverage buckets\n","merged['lev_bucket'] = pd.cut(merged['leverage'].replace(0, np.nan),\n","                             bins=[0,1,3,5,10,20,100,1e9],\n","                             labels=['0-1','1-3','3-5','5-10','10-20','20-100','100+'],\n","                             include_lowest=True)\n","\n","# per-account aggregates\n","if 'account' in merged.columns:\n","    acct_agg = merged.groupby('account').agg(\n","        trades_count=('pnl','count'),\n","        total_pnl=('pnl','sum'),\n","        mean_pnl=('pnl','mean'),\n","        win_rate=('win','mean'),\n","        avg_leverage=('leverage','mean')\n","    ).reset_index()\n","else:\n","    acct_agg = pd.DataFrame()\n","\n","# per-sentiment\n","sent_agg = merged.groupby('classification').agg(\n","    trades_count=('pnl','count'),\n","    total_pnl=('pnl','sum'),\n","    mean_pnl=('pnl','mean'),\n","    win_rate=('win','mean'),\n","    avg_leverage=('leverage','mean')\n",").reset_index()\n","\n","# -----------------------------\n","# 6) Exploratory plots (matplotlib/seaborn) -> save PNG files & also create base64 for embedding\n","# -----------------------------\n","def save_fig_and_b64(fig, filename, outdir=out_dir, dpi=150):\n","    path = os.path.join(outdir, filename)\n","    fig.savefig(path, bbox_inches='tight', dpi=dpi)\n","    buf = BytesIO()\n","    fig.savefig(buf, format='png', bbox_inches='tight', dpi=dpi)\n","    buf.seek(0)\n","    b64 = base64.b64encode(buf.read()).decode('utf-8')\n","    plt.close(fig)\n","    return path, b64\n","\n","# a) Boxplot: PnL by sentiment (exclude Unknown for clarity)\n","fig1 = plt.figure(figsize=(10,6))\n","ax1 = sns.boxplot(x='classification', y='pnl', data=merged[merged['classification']!='Unknown'].replace([np.inf, -np.inf], np.nan).dropna(subset=['pnl']))\n","ax1.set_title('PnL distribution by Market Sentiment')\n","ax1.set_xlabel('Sentiment')\n","ax1.set_ylabel('PnL')\n","path1, b64_1 = save_fig_and_b64(fig1, 'pnl_by_sentiment.png')\n","\n","# b) Win rate by sentiment (bar)\n","wr = merged[merged['classification']!='Unknown'].groupby('classification').agg(win_rate=('win','mean')).reset_index()\n","fig2 = plt.figure(figsize=(8,5))\n","ax2 = sns.barplot(x='classification', y='win_rate', data=wr)\n","ax2.set_title('Win Rate by Sentiment')\n","ax2.set_ylabel('Win Rate')\n","path2, b64_2 = save_fig_and_b64(fig2, 'win_rate_by_sentiment.png')\n","\n","# c) Mean PnL by leverage bucket and sentiment\n","pivot = merged[merged['classification']!='Unknown'].groupby(['lev_bucket','classification']).agg(mean_pnl=('pnl','mean')).reset_index()\n","fig3 = plt.figure(figsize=(12,6))\n","ax3 = sns.pointplot(x='lev_bucket', y='mean_pnl', hue='classification', data=pivot, dodge=True)\n","ax3.set_title('Mean PnL by Leverage Bucket and Sentiment')\n","ax3.set_xlabel('Leverage Bucket')\n","ax3.set_ylabel('Mean PnL')\n","path3, b64_3 = save_fig_and_b64(fig3, 'pnl_by_leverage_sentiment.png')\n","\n","# d) Time series: rolling 7-day total pnl\n","daily = merged.groupby('trade_date').agg(total_pnl=('pnl','sum'), trades=('pnl','count')).reset_index()\n","daily['trade_date'] = pd.to_datetime(daily['trade_date'])\n","daily = daily.sort_values('trade_date')\n","daily['rolling_pnl_7d'] = daily['total_pnl'].rolling(7, min_periods=1).mean()\n","fig4 = plt.figure(figsize=(12,5))\n","plt.plot(daily['trade_date'], daily['rolling_pnl_7d'], label='7-day rolling total PnL')\n","plt.title('7-day Rolling Total PnL across all traders')\n","plt.xlabel('Date')\n","plt.ylabel('Total PnL (7d rolling)')\n","plt.legend()\n","path4, b64_4 = save_fig_and_b64(fig4, 'rolling_pnl.png')\n","\n","# Also create small interactive Plotly HTML (optional saved)\n","try:\n","    fig_plotly = px.box(merged[merged['classification']!='Unknown'], x='classification', y='pnl', title='PnL by Sentiment (interactive)')\n","    plotly_html_path = os.path.join(out_dir, 'plotly_pnl_by_sentiment.html')\n","    fig_plotly.write_html(plotly_html_path)\n","except Exception as e:\n","    print(\"Failed to produce plotly interactive HTML:\", e)\n","    plotly_html_path = None\n","\n","# -----------------------------\n","# 7) Statistical tests & simple predictive model\n","# -----------------------------\n","# T-test PnL Fear vs Greed (only if both exist)\n","fear_pnl = merged[merged['classification']=='Fear']['pnl'].dropna()\n","greed_pnl = merged[merged['classification']=='Greed']['pnl'].dropna()\n","ttest_res = None\n","if len(fear_pnl) > 1 and len(greed_pnl) > 1:\n","    ttest_res = stats.ttest_ind(fear_pnl, greed_pnl, equal_var=False)\n","    ttest_stat, ttest_p = ttest_res.statistic, ttest_res.pvalue\n","else:\n","    ttest_stat, ttest_p = np.nan, np.nan\n","\n","# Chi2: sentiment vs win (use top two sentiments if many)\n","try:\n","    chi2_res = stats.chi2_contingency(pd.crosstab(merged['classification'], merged['win']))\n","except Exception as e:\n","    chi2_res = None\n","\n","# Simple model: predict win using [is_long, leverage, size, sentiment_onehots]\n","model_info = {}\n","model_df = merged[merged['classification'].isin(['Fear','Greed'])].copy()\n","if len(model_df) >= 50:\n","    model_df['is_long'] = (model_df['side']=='buy').astype(int)\n","    X = model_df[['is_long','leverage','size']].fillna(0)\n","    X = pd.concat([X, pd.get_dummies(model_df['classification'], prefix='sent')], axis=1)\n","    y = model_df['win']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n","    scaler = StandardScaler()\n","    X_train_s = scaler.fit_transform(X_train)\n","    X_test_s = scaler.transform(X_test)\n","    clf = LogisticRegression(max_iter=2000)\n","    clf.fit(X_train_s, y_train)\n","    y_pred = clf.predict(X_test_s)\n","    y_proba = clf.predict_proba(X_test_s)[:,1]\n","    report = classification_report(y_test, y_pred, output_dict=True)\n","    roc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test))>1 else np.nan\n","    coefs = pd.DataFrame({'feature': X.columns, 'coef': clf.coef_[0]})\n","    model_info = {\n","        'classification_report': report,\n","        'roc_auc': roc,\n","        'coefficients': coefs\n","    }\n","else:\n","    print(\"Not enough data for predictive model (need >=50 rows with Fear/Greed).\")\n","\n","# -----------------------------\n","# 8) Build HTML report with embedded images (base64) + tables\n","# -----------------------------\n","def df_to_html_table(df, title=None):\n","    html = \"\"\n","    if title:\n","        html += f\"<h3>{title}</h3>\\n\"\n","    html += df.to_html(index=False, classes='table', border=0)\n","    return html\n","\n","html_parts = []\n","html_parts.append(\"<html><head><meta charset='utf-8'><title>Trader vs Sentiment Analysis</title>\")\n","html_parts.append(\"\"\"<style>\n","body { font-family: Arial, Helvetica, sans-serif; margin: 30px; color: #222; }\n","h1 { color: #0B5FFF; }\n",".table { border-collapse: collapse; width: 100%; margin-bottom: 20px;}\n",".table th, .table td { border: 1px solid #ddd; padding: 8px; }\n",".section { margin-bottom: 30px; }\n",".small { font-size: 0.9em; color: #555; }\n","</style>\"\"\")\n","html_parts.append(\"</head><body>\")\n","html_parts.append(f\"<h1>Trader vs Market Sentiment Analysis</h1>\")\n","html_parts.append(f\"<p class='small'>Generated: {datetime.utcnow().isoformat()} UTC</p>\")\n","\n","# Overview\n","html_parts.append(\"<div class='section'><h2>Overview</h2>\")\n","html_parts.append(f\"<p>Number of trades analyzed: <strong>{len(merged):,}</strong></p>\")\n","html_parts.append(\"<p>Sentiment distribution:</p>\")\n","html_parts.append(\"<pre>\" + merged['classification'].value_counts().to_string() + \"</pre>\")\n","html_parts.append(\"</div>\")\n","\n","# Plots\n","html_parts.append(\"<div class='section'><h2>Plots</h2>\")\n","html_parts.append(\"<h3>PnL distribution by sentiment</h3>\")\n","html_parts.append(f\"<img src='data:image/png;base64,{b64_1}' alt='PnL by Sentiment' style='max-width:100%;height:auto;border:1px solid #ccc;padding:4px;'/>\")\n","html_parts.append(\"<h3>Win Rate by Sentiment</h3>\")\n","html_parts.append(f\"<img src='data:image/png;base64,{b64_2}' alt='Win Rate' style='max-width:100%;height:auto;border:1px solid #ccc;padding:4px;'/>\")\n","html_parts.append(\"<h3>Mean PnL by Leverage Bucket and Sentiment</h3>\")\n","html_parts.append(f\"<img src='data:image/png;base64,{b64_3}' alt='Leverage vs PnL' style='max-width:100%;height:auto;border:1px solid #ccc;padding:4px;'/>\")\n","html_parts.append(\"<h3>7-day rolling total PnL</h3>\")\n","html_parts.append(f\"<img src='data:image/png;base64,{b64_4}' alt='Rolling PnL' style='max-width:100%;height:auto;border:1px solid #ccc;padding:4px;'/>\")\n","if plotly_html_path:\n","    html_parts.append(f\"<p>Interactive Plotly chart saved separately: {os.path.basename(plotly_html_path)}</p>\")\n","html_parts.append(\"</div>\")\n","\n","# Tables & stats\n","html_parts.append(\"<div class='section'><h2>Aggregates & Statistics</h2>\")\n","html_parts.append(df_to_html_table(sent_agg, title=\"Per-Sentiment Aggregates\"))\n","if not acct_agg.empty:\n","    html_parts.append(df_to_html_table(acct_agg.sort_values('total_pnl', ascending=False).head(50), title=\"Top 50 accounts by total PnL\"))\n","html_parts.append(\"<h3>Statistical tests</h3>\")\n","html_parts.append(\"<ul>\")\n","html_parts.append(f\"<li>T-test (Fear vs Greed) on PnL: statistic={ttest_stat}, p-value={ttest_p}</li>\")\n","if chi2_res is not None:\n","    html_parts.append(f\"<li>Chi2 contingency (classification vs win): chi2_stat={chi2_res[0]:.4f}, p={chi2_res[1]:.4g}</li>\")\n","else:\n","    html_parts.append(\"<li>Chi2 contingency could not be computed (insufficient data).</li>\")\n","html_parts.append(\"</ul>\")\n","html_parts.append(\"</div>\")\n","\n","# Model summary if available\n","if model_info:\n","    html_parts.append(\"<div class='section'><h2>Predictive model (Logistic Regression)</h2>\")\n","    html_parts.append(f\"<p>ROC AUC: {model_info['roc_auc']:.4f}</p>\")\n","    # coefficients table\n","    html_parts.append(df_to_html_table(model_info['coefficients'].sort_values('coef', key=abs, ascending=False), title=\"Model coefficients\"))\n","    html_parts.append(\"</div>\")\n","\n","# Footer\n","html_parts.append(\"<div class='section small'><p>Notes: This report was generated automatically. Check the saved CSV files for raw+aggregated outputs.</p></div>\")\n","\n","html_parts.append(\"</body></html>\")\n","report_html = \"\\n\".join(html_parts)\n","\n","# Save HTML\n","html_path = os.path.join(out_dir, 'analysis_report.html')\n","with open(html_path, 'w', encoding='utf-8') as f:\n","    f.write(report_html)\n","print(\"Saved HTML report to:\", html_path)\n","\n","# -----------------------------\n","# 9) Convert HTML -> PDF (WeasyPrint if available, else ReportLab fallback)\n","# -----------------------------\n","pdf_path = os.path.join(out_dir, 'analysis_report.pdf')\n","if use_weasy:\n","    try:\n","        HTML(string=report_html).write_pdf(pdf_path)\n","        print(\"Saved PDF report via WeasyPrint to:\", pdf_path)\n","    except Exception as e:\n","        print(\"WeasyPrint failed to generate PDF, falling back to ReportLab. Error:\", e)\n","        use_weasy = False\n","\n","if not use_weasy:\n","    # Fallback: create a multi-page PDF using ReportLab by laying out the HTML contents and images.\n","    # We'll place text summary + each saved PNG on its own page.\n","    try:\n","        c = canvas.Canvas(pdf_path, pagesize=letter)\n","        width, height = letter\n","        # Title page\n","        c.setFont(\"Helvetica-Bold\", 18)\n","        c.drawString(40, height-80, \"Trader vs Market Sentiment Analysis\")\n","        c.setFont(\"Helvetica\", 10)\n","        c.drawString(40, height-100, f\"Generated: {datetime.utcnow().isoformat()} UTC\")\n","        c.drawString(40, height-120, f\"Number of trades analyzed: {len(merged):,}\")\n","        c.showPage()\n","        # Add Sentiment table as text (first 80 rows)\n","        c.setFont(\"Helvetica-Bold\", 14)\n","        c.drawString(40, height-40, \"Per-Sentiment Aggregates (top rows):\")\n","        c.setFont(\"Helvetica\", 9)\n","        text = c.beginText(40, height-60)\n","        sent_str = sent_agg.head(80).to_string(index=False)\n","        for line in sent_str.splitlines():\n","            text.textLine(line)\n","        c.drawText(text)\n","        c.showPage()\n","        # Add each PNG as a full-page image\n","        for imgfile in [path1, path2, path3, path4]:\n","            try:\n","                img = Image.open(imgfile)\n","                # preserve aspect ratio fit to page with margins\n","                maxw = width - 80\n","                maxh = height - 120\n","                img_w, img_h = img.size\n","                ratio = min(maxw/img_w, maxh/img_h)\n","                draw_w = img_w * ratio\n","                draw_h = img_h * ratio\n","                # save to temp file as JPEG to ensure compatibility\n","                tmp_path = imgfile + \".tmp.jpg\"\n","                img.convert('RGB').save(tmp_path, \"JPEG\")\n","                c.drawImage(tmp_path, 40, (height - 60 - draw_h), width=draw_w, height=draw_h)\n","                c.showPage()\n","                os.remove(tmp_path)\n","            except Exception as e:\n","                print(\"Failed to add image to PDF fallback:\", e)\n","        c.save()\n","        print(\"Saved PDF report via ReportLab fallback to:\", pdf_path)\n","    except Exception as e:\n","        print(\"ReportLab fallback failed. Error:\", e)\n","        print(\"At least the HTML report was saved at:\", html_path)\n","\n","# -----------------------------\n","# 10) Save CSV outputs, model artifacts, and Streamlit app script\n","# -----------------------------\n","merged_path = os.path.join(out_dir, 'merged_trades_sentiment.csv')\n","merged.to_csv(merged_path, index=False)\n","sent_agg.to_csv(os.path.join(out_dir, 'sentiment_aggregates.csv'), index=False)\n","acct_path = os.path.join(out_dir, 'account_aggregates.csv')\n","if not acct_agg.empty:\n","    acct_agg.to_csv(acct_path, index=False)\n","if model_info:\n","    model_info['coefficients'].to_csv(os.path.join(out_dir, 'model_coefficients.csv'), index=False)\n","\n","print(\"Saved merged CSV to:\", merged_path)\n","if not acct_agg.empty:\n","    print(\"Saved account aggregates to:\", acct_path)\n","\n","# Streamlit app (simple)\n","streamlit_code = f\"\"\"# Streamlit dashboard for Trader vs Sentiment\n","import streamlit as st\n","import pandas as pd\n","import plotly.express as px\n","\n","st.set_page_config(page_title='Trader vs Sentiment Dashboard', layout='wide')\n","st.title('Trader vs Sentiment Dashboard')\n","\n","merged = pd.read_csv('{merged_path}')\n","\n","st.sidebar.header('Filters')\n","sent_filter = st.sidebar.multiselect('Sentiment', merged['classification'].unique().tolist(), default=merged['classification'].unique().tolist())\n","\n","df = merged[merged['classification'].isin(sent_filter)]\n","\n","st.header('Summary')\n","st.metric('Trades', f\\\"{{len(df):,}}\\\")\n","st.metric('Avg PnL', f\\\"{{df['pnl'].mean():.4f}}\\\")\n","\n","st.header('PnL by Sentiment (boxplot)')\n","fig = px.box(df[df['classification']!='Unknown'], x='classification', y='pnl', points='outliers')\n","st.plotly_chart(fig, use_container_width=True)\n","\n","st.header('Leverage distribution by Sentiment')\n","fig2 = px.histogram(df, x='leverage', color='classification', nbins=30)\n","st.plotly_chart(fig2, use_container_width=True)\n","\n","st.header('Per-account table (top 50 by total PnL)')\n","if 'account' in df.columns:\n","    acct = df.groupby('account').agg(trades=('pnl','count'), total_pnl=('pnl','sum'), win_rate=('win','mean')).reset_index().sort_values('total_pnl', ascending=False).head(50)\n","    st.dataframe(acct)\n","else:\n","    st.info('No account column present in dataset.')\n","\"\"\"\n","\n","streamlit_file = os.path.join(out_dir, 'dashboard_app.py')\n","with open(streamlit_file, 'w') as f:\n","    f.write(streamlit_code)\n","print(\"Saved Streamlit app script to:\", streamlit_file)\n","print(\"To run the dashboard locally or in Colab (if supported), run:\")\n","print(f\"  streamlit run {streamlit_file}\")\n","\n","# -----------------------------\n","# 11) Final message & summary\n","# -----------------------------\n","print(\"\\n=== Finished ===\")\n","print(\"Outputs saved to directory:\", out_dir)\n","print(\"Key files:\")\n","print(\" - HTML report:\", html_path)\n","print(\" - PDF report:\", pdf_path)\n","print(\" - Merged CSV:\", merged_path)\n","print(\" - Sentiment aggregates CSV:\", os.path.join(out_dir, 'sentiment_aggregates.csv'))\n","print(\" - Streamlit app:\", streamlit_file)\n","if plotly_html_path:\n","    print(\" - Interactive Plotly HTML:\", plotly_html_path)\n","print(\"\\nOpen the HTML report to inspect visuals immediately. If PDF conversion fails, you still have the HTML and PNG images.\")"]}]}